{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sakamoto_bert_with_sst-2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok440ey5qnb1"
      },
      "source": [
        "# Sakamoto BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NOgl11vveW"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_SY2z6yPzsR"
      },
      "source": [
        "install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n8sEYakyjpw"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52OMOpp4tATI"
      },
      "source": [
        "download the BERT Japanese pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf5d3JaoqNr7"
      },
      "source": [
        "!wget http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JapaneseBertPretrainedModel/Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5MTP4HvAEk"
      },
      "source": [
        "!unzip Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers.zip -d bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KgFeuTjoU7i"
      },
      "source": [
        "copy Sakamoto SST-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz4lZhZet20r"
      },
      "source": [
        "!cp -r drive/My\\ Drive/sakamoto-bert/SST-2 sakamoto_sst2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROJKeU4ev3Mi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCZTi7Qe_Ur4"
      },
      "source": [
        "clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfc7KcJHzwKo"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kSf4cG-vARX"
      },
      "source": [
        "%cd transformers\n",
        "!git checkout bb9559a7f98fc18747fd957c3bd2a6e4c1111e45\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yz3FiaDqA-m"
      },
      "source": [
        "fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVgKfjQawoSW"
      },
      "source": [
        "!python transformers/examples/text-classification/run_glue.py \\\n",
        "    --model_name_or_path bert/Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers \\\n",
        "    --task_name SST-2 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --data_dir sakamoto_sst2 \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_device_train_batch_size 32 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --output_dir bert_sakamoto_sst2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zbL0W_Bxfps"
      },
      "source": [
        "copy the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUuMg0rNw_dW"
      },
      "source": [
        "!cp -r bert_sakamoto_sst2 drive/My\\ Drive/sakamoto-bert/model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J8uICRa5n3t"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMIleRijDl9-"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjAXSOsX7mqT"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert_sakamoto_sst2')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert_sakamoto_sst2', return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yx2wS09oNg9"
      },
      "source": [
        "classes = ['not sksk_sskn', 'is sksk_sskn']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlOF-Bd5-Tes"
      },
      "source": [
        "import csv\n",
        "\n",
        "sequences = []\n",
        "with open('sakamoto_sst2/test.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        sequences.append(row['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJUIrVZ_Aw-n"
      },
      "source": [
        "for sequence in sequences:\n",
        "    sequence_tensor = tokenizer(sequence, return_tensors='pt')\n",
        "    classification_logits = model(**sequence_tensor).logits\n",
        "    results = torch.softmax(classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "    print(sequence)\n",
        "    for i in range(len(classes)):\n",
        "        print(f'{classes[i]}: {results[i] * 100}%')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPUb7qdNVPSq"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqmY3OCDxAhS"
      },
      "source": [
        "Transformers\n",
        "\n",
        "- [transformers/examples/text-classification at master · huggingface/transformers](https://github.com/huggingface/transformers/tree/master/examples/text-classification)\n",
        "- [Summary of the tasks — transformers 3.0.2 documentation](https://huggingface.co/transformers/task_summary.html#sequence-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajvro66ztMFn"
      },
      "source": [
        "BERT Japanese Pretrained Model\n",
        "\n",
        "- [BERT日本語Pretrainedモデル - KUROHASHI-CHU-MURAWAKI LAB](http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OXimgQCf5L0"
      },
      "source": [
        "The Stanford Sentiment Treebank\t\n",
        "\n",
        "- [GLUE Benchmark](https://gluebenchmark.com/tasks)"
      ]
    }
  ]
}